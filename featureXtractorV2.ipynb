{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# set path to shape file\n",
    "# shape_file_path = '/mnt/d/education/HSI/aspirantura/CAMELS_ru/files/openf_gauges_watersheds/watersheds_openf.shp'\n",
    "shape_file_path = '/home/dima/Documents/education/HSI/aspirantura/Dissertation/conus_data/basin_set_full_res/HCDN_nhru_final_671.shp'\n",
    "# set path to downloaded HydroATLAS\n",
    "gdb_file_path = '/home/dima/Documents/education/HSI/aspirantura/Dissertation/files/BasinATLAS/BasinATLAS_v10.gdb/'\n",
    "# set path where results will be stored\n",
    "path_to_save = '/home/dima/Documents/education/HSI/aspirantura/Dissertation/conus_data/featureXtractor'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Read shape file with geometry column\n",
    "big_shape = gpd.read_file(shape_file_path)\n",
    "big_shape = big_shape[['hru_id', 'geometry', 'AREA']]\n",
    "\n",
    "# rename column of gauge identification number to capital ID\n",
    "big_shape = big_shape.rename(columns={\"hru_id\": \"gauge_id\"})\n",
    "big_shape['gauge_id'] = ['0'+i if len(i) != 8\n",
    "                         else i\n",
    "                         for i in map(str, big_shape['gauge_id'])]\n",
    "big_shape\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauge_id</th>\n",
       "      <th>geometry</th>\n",
       "      <th>AREA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01013500</td>\n",
       "      <td>MULTIPOLYGON (((-68.35650 46.90311, -68.35612 ...</td>\n",
       "      <td>2.303988e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01022500</td>\n",
       "      <td>POLYGON ((-67.97836 44.61310, -67.97800 44.613...</td>\n",
       "      <td>6.203873e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01030500</td>\n",
       "      <td>MULTIPOLYGON (((-67.83991 45.36614, -67.83955 ...</td>\n",
       "      <td>3.676155e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01031500</td>\n",
       "      <td>MULTIPOLYGON (((-69.33810 45.12317, -69.33800 ...</td>\n",
       "      <td>7.665447e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01047000</td>\n",
       "      <td>POLYGON ((-70.10847 45.21669, -70.10858 45.216...</td>\n",
       "      <td>9.049562e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>14309500</td>\n",
       "      <td>POLYGON ((-123.81322 42.89103, -123.81312 42.8...</td>\n",
       "      <td>2.263143e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>14316700</td>\n",
       "      <td>POLYGON ((-122.49936 43.47688, -122.49972 43.4...</td>\n",
       "      <td>5.880250e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>14325000</td>\n",
       "      <td>POLYGON ((-124.07751 42.89822, -124.07716 42.8...</td>\n",
       "      <td>4.449257e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>14362250</td>\n",
       "      <td>POLYGON ((-123.15128 42.19624, -123.15118 42.1...</td>\n",
       "      <td>4.387790e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>14400000</td>\n",
       "      <td>POLYGON ((-124.02084 42.36178, -124.02073 42.3...</td>\n",
       "      <td>7.033865e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>671 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gauge_id                                           geometry          AREA\n",
       "0    01013500  MULTIPOLYGON (((-68.35650 46.90311, -68.35612 ...  2.303988e+09\n",
       "1    01022500  POLYGON ((-67.97836 44.61310, -67.97800 44.613...  6.203873e+08\n",
       "2    01030500  MULTIPOLYGON (((-67.83991 45.36614, -67.83955 ...  3.676155e+09\n",
       "3    01031500  MULTIPOLYGON (((-69.33810 45.12317, -69.33800 ...  7.665447e+08\n",
       "4    01047000  POLYGON ((-70.10847 45.21669, -70.10858 45.216...  9.049562e+08\n",
       "..        ...                                                ...           ...\n",
       "666  14309500  POLYGON ((-123.81322 42.89103, -123.81312 42.8...  2.263143e+08\n",
       "667  14316700  POLYGON ((-122.49936 43.47688, -122.49972 43.4...  5.880250e+08\n",
       "668  14325000  POLYGON ((-124.07751 42.89822, -124.07716 42.8...  4.449257e+08\n",
       "669  14362250  POLYGON ((-123.15128 42.19624, -123.15118 42.1...  4.387790e+07\n",
       "670  14400000  POLYGON ((-124.02084 42.36178, -124.02073 42.3...  7.033865e+08\n",
       "\n",
       "[671 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def polygon_area(lats: list, lons: list, radius=6378137):\n",
    "    \"\"\"\n",
    "    Computes area of spherical polygon, assuming spherical Earth. \n",
    "    Returns result in ratio of the sphere's area if the radius is specified.\n",
    "    Otherwise, in the units of provided radius.\n",
    "    lats and lons are in degrees.\n",
    "\n",
    "    Args:\n",
    "        lats (list): list of latitudinal coordinates\n",
    "        lons (list): list of longitudinal coordinates\n",
    "        radius (int, optional): Earth radius. Defaults to 6378137.\n",
    "\n",
    "    Returns:\n",
    "        area: area of object in sq. km\n",
    "    \"\"\"\n",
    "    from numpy import arctan2, cos, sin, sqrt, pi, power, append, diff, deg2rad\n",
    "    lats, lons = np.deg2rad(lats), np.deg2rad(lons)\n",
    "\n",
    "    # Line integral based on Green's Theorem, assumes spherical Earth\n",
    "\n",
    "    #close polygon\n",
    "    if lats[0] != lats[-1]:\n",
    "        lats = append(lats, lats[0])\n",
    "        lons = append(lons, lons[0])\n",
    "\n",
    "    #colatitudes relative to (0,0)\n",
    "    a = sin(lats/2)**2 + cos(lats) * sin(lons/2)**2\n",
    "    colat = 2*arctan2(sqrt(a), sqrt(1-a))\n",
    "\n",
    "    #azimuths relative to (0,0)\n",
    "    az = arctan2(cos(lats) * sin(lons), sin(lats)) % (2*pi)\n",
    "\n",
    "    # Calculate diffs\n",
    "    # daz = diff(az) % (2*pi)\n",
    "    daz = diff(az)\n",
    "    daz = (daz + pi) % (2 * pi) - pi\n",
    "\n",
    "    deltas = diff(colat)/2\n",
    "    colat = colat[0:-1]+deltas\n",
    "\n",
    "    # Perform integral\n",
    "    integrands = (1-cos(colat)) * daz\n",
    "\n",
    "    # Integrate\n",
    "    area = abs(sum(integrands))/(4*pi)\n",
    "\n",
    "    area = min(area, 1-area)\n",
    "    if radius is not None:  # return in units of radius\n",
    "        return area * 4 * pi * radius**2 / 10**6\n",
    "    else:  # return in ratio of sphere total area\n",
    "        return area / 10**6\n",
    "\n",
    "\n",
    "def select_big_from_MP(WS_geometry):\n",
    "    \"\"\"\n",
    "    Select biggest polygon from MultiPolygon object.\n",
    "    Needs to eliminate fake structures out of basin shape\n",
    "\n",
    "    Args:\n",
    "        WS_geometry (Geometry): Desired basin shape\n",
    "\n",
    "    Returns:\n",
    "        Ws_geometry: Biggest polygon which correspond to natural form\n",
    "    \"\"\"\n",
    "    if type(WS_geometry) == MultiPolygon:\n",
    "        big_area = [polygon_area(lats=polygon.exterior.coords.xy[1],\n",
    "                                 lons=polygon.exterior.coords.xy[0])\n",
    "                    for polygon in WS_geometry]\n",
    "        WS_geometry = WS_geometry[np.argmax(big_area)]\n",
    "    else:\n",
    "        WS_geometry = WS_geometry\n",
    "    return WS_geometry\n",
    "\n",
    "\n",
    "def get_gdf_Poly(some_geometry: Polygon):\n",
    "    \"\"\"\n",
    "    Transform Polygon object to GeoDataFrame with EPSG projection\n",
    "\n",
    "    Args:\n",
    "        some_geometry (Polygon): Polygon from GeoDataFrame of Polygon's\n",
    "\n",
    "    Returns:\n",
    "        target (GeoDataFrame): Polygon stored in GeoDataFrame format\n",
    "    \"\"\"\n",
    "    target = gpd.GeoSeries(\n",
    "        select_big_from_MP(some_geometry))\n",
    "    target = gpd.GeoDataFrame({'geometry': target}).set_crs('EPSG:4326')\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def find_POLY_area(poly: GeoDataFrame):\n",
    "    \"\"\"\n",
    "    Calculates area of shape stored in GeoDataFrame\n",
    "\n",
    "    Args:\n",
    "        poly (GeoDataFrame): Desired shape\n",
    "\n",
    "    Returns:\n",
    "        area (float): area of object in sq. km\n",
    "    \"\"\"\n",
    "    poly = select_big_from_MP(poly['geometry'][0])\n",
    "    area = polygon_area(lats=poly.exterior.xy[1],\n",
    "                        lons=poly.exterior.xy[0])\n",
    "    return area\n",
    "\n",
    "\n",
    "def parallelize_function(WS: GeoDataFrame, path_to_HydroATLAS: str):\n",
    "    \"\"\"\n",
    "    This function generate list of tuples\n",
    "    where each tuple stands for row in DF\n",
    "    of watersheds\n",
    "\n",
    "    Args:\n",
    "        WS (GeoDataFrame): GeoDataFrame of desired watersheds\n",
    "        path_to_HydroATLAS (str): path to gdb of HydroATLAS on disk\n",
    "        layer_small (fiona): [description]\n",
    "\n",
    "    Returns:\n",
    "        mp_tuples (tuple): tuple of values which will be required for \n",
    "        parallel launch\n",
    "    \"\"\"\n",
    "    mp_tuples = list()\n",
    "\n",
    "    for row in range(len(WS)):\n",
    "        mp_tuples.append((WS.loc[row, 'geometry'],\n",
    "                          path_to_HydroATLAS))\n",
    "\n",
    "    return mp_tuples\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Necessary columns to work with HydroATLAS\n",
    "monthes = ['01', '02', '03', '04', '05', '06',\n",
    "           '07', '08', '09', '10', '11', '12']\n",
    "lc_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09',\n",
    "              '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
    "                    '19', '20', '21', '22']\n",
    "wetland_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
    "hydrology_variables = [item for sublist in [['inu_pc_ult'],\n",
    "                                            ['lka_pc_use'],\n",
    "                                            ['lkv_mc_usu'],\n",
    "                                            ['rev_mc_usu'],\n",
    "                                            ['dor_pc_pva'],\n",
    "                                            ['gwt_cm_sav']]\n",
    "                       for item in sublist]\n",
    "physiography_variables = [item for sublist in [['ele_mt_sav'],\n",
    "                                               ['slp_dg_sav'],\n",
    "                                               ['sgr_dk_sav']]\n",
    "                          for item in sublist]\n",
    "climate_variables = [item for sublist\n",
    "                     in [['clz_cl_smj'],\n",
    "                         ['cls_cl_smj'],\n",
    "                         ['tmp_dc_s{}'.format(i) for i in monthes],\n",
    "                         ['pre_mm_s{}'.format(i) for i in monthes],\n",
    "                         ['pet_mm_s{}'.format(i) for i in monthes],\n",
    "                         ['aet_mm_s{}'.format(i) for i in monthes],\n",
    "                         ['ari_ix_sav'],\n",
    "                         ['cmi_ix_s{}'.format(i) for i in monthes],\n",
    "                         ['snw_pc_s{}'.format(i) for i in monthes]]\n",
    "                     for item in sublist]\n",
    "landcover_variables = [item for sublist\n",
    "                       in [['glc_cl_smj'],\n",
    "                           ['glc_pc_s{}'.format(i) for i in lc_classes],\n",
    "                           ['wet_pc_s{}'.format(i)\n",
    "                            for i in wetland_classes],\n",
    "                           ['for_pc_sse'],\n",
    "                           ['crp_pc_sse'],\n",
    "                           ['pst_pc_sse'],\n",
    "                           ['ire_pc_sse'],\n",
    "                           ['gla_pc_sse'],\n",
    "                           ['prm_pc_sse']]\n",
    "                       for item in sublist]\n",
    "soil_and_geo_variables = [item for sublist\n",
    "                          in [['cly_pc_sav'],\n",
    "                              ['slt_pc_sav'],\n",
    "                              ['snd_pc_sav'],\n",
    "                              ['soc_th_sav'],\n",
    "                              ['swc_pc_syr'],\n",
    "                              ['swc_pc_s{}'.format(i) for i in monthes],\n",
    "                              ['kar_pc_sse'],\n",
    "                              ['ero_kh_sav']]\n",
    "                          for item in sublist]\n",
    "urban_variables = [item for sublist in [['urb_pc_sse'],\n",
    "                                        ['hft_ix_s93'],\n",
    "                                        ['hft_ix_s09']]\n",
    "                   for item in sublist]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def featureXtractor(user_ws: Polygon, gdb_file_path: str):\n",
    "    \"\"\"\n",
    "    Fucntion calculates weighted mean of variables which are occured\n",
    "    in intersection of user_ws and subbasins from HydroATLAS database\n",
    "\n",
    "    Args:\n",
    "        user_ws (Polygon): User's watershed boundary\n",
    "        gdb_file_path (str): path to gdb of HydroATLAS on disk\n",
    "\n",
    "    Returns:\n",
    "        geo_vector (Series): Series of variables corresponded to user_ws id\n",
    "    \"\"\"\n",
    "\n",
    "    # get only biggest polygon areas from watershed\n",
    "    gdf_your_WS = select_big_from_MP(user_ws)\n",
    "\n",
    "    # transform to geopandas for geometry operations\n",
    "    gdf_your_WS = gpd.GeoSeries([gdf_your_WS])\n",
    "    gdf_your_WS = gpd.GeoDataFrame({'geometry': gdf_your_WS})\n",
    "    gdf_your_WS = gdf_your_WS.set_crs('EPSG:4326')\n",
    "\n",
    "    # connect to HydroATLAS file with fiona+gpd interface\n",
    "    layer_small = fiona.listlayers(gdb_file_path)[-1]\n",
    "    # Read choosen geodatabase layer with geopandas\n",
    "    gdf = gpd.read_file(gdb_file_path,\n",
    "                        mask=user_ws,\n",
    "                        layer=layer_small,\n",
    "                        ignore_geometry=False)\n",
    "    # create new column where each intersection is geodataframe\n",
    "    gdf['gdf_geometry'] = tuple(map(get_gdf_Poly, gdf['geometry']))\n",
    "    # calculate weight of each intersection correspond to it native size\n",
    "    gdf['weights'] = gdf['gdf_geometry'].apply(\n",
    "        lambda x: gpd.overlay(gdf_your_WS, x)).apply(find_POLY_area) /\\\n",
    "        gdf['gdf_geometry'].apply(find_POLY_area)\n",
    "    # calculate area with weight appliance\n",
    "    gdf['weight_area'] = tuple(map(find_POLY_area,\n",
    "                                   gdf['gdf_geometry'])) * gdf['weights']\n",
    "\n",
    "    # calculate each variable weighted mean\n",
    "    geo_vector = gdf[hydrology_variables +\n",
    "                     physiography_variables +\n",
    "                     climate_variables +\n",
    "                     landcover_variables +\n",
    "                     soil_and_geo_variables +\n",
    "                     urban_variables].apply(\n",
    "        lambda x:\n",
    "            np.sum(x * gdf['weights'])/np.sum(gdf['weights']))\n",
    "    # some values in HydroATLAS was multiplied by <X>, so to bring it\n",
    "    # back to original form this procedure is required\n",
    "    divide_by_10 = [item for sublist\n",
    "                    in [['lka_pc_use'],\n",
    "                        ['dor_pc_pva'],\n",
    "                        ['slp_dg_sav'],\n",
    "                        ['tmp_dc_s{}'.format(i) for i in monthes],\n",
    "                        ['hft_ix_s93'],\n",
    "                        ['hft_ix_s09']]\n",
    "                    for item in sublist]\n",
    "    divide_by_100 = [item for sublist\n",
    "                     in [['ari_ix_sav'],\n",
    "                         ['cmi_ix_s{}'.format(i) for i in monthes]]\n",
    "                     for item in sublist]\n",
    "\n",
    "    geo_vector[divide_by_10] /= 10\n",
    "    geo_vector[divide_by_100] /= 100\n",
    "    # store basin area\n",
    "    geo_vector['ws_area'] = find_POLY_area(gdf_your_WS)\n",
    "\n",
    "    return geo_vector\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "data = parallelize_function(WS=big_shape,\n",
    "                            path_to_HydroATLAS=gdb_file_path)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# prepare data and iterations on test example\n",
    "# get count of cpu. Substract by 2 to not to overwhelm system\n",
    "function_processors = mp.cpu_count()//2\n",
    "process_pool = mp.Pool(function_processors)\n",
    "# WS, WS_index, path_to_HydroATLAS, layers_from_HydroATLAS\n",
    "output = process_pool.starmap(featureXtractor, tq.tqdm(data))\n",
    "process_pool.close()\n",
    "process_pool.join()\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbb0f5082a3449e1b5f2bd2c43994473"
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def save_results(eXtracted_data: list, gauge_ids: list, path_to_save: str):\n",
    "\n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.makedirs(path_to_save)\n",
    "\n",
    "    # create DataFrame to save it by categories\n",
    "    df_to_disk = pd.DataFrame(data=eXtracted_data,\n",
    "                              index=gauge_ids)\n",
    "\n",
    "    save_names = {'hydro': hydrology_variables,\n",
    "                  'physio': physiography_variables+['ws_area'],\n",
    "                  'climate': climate_variables,\n",
    "                  'landcover': landcover_variables,\n",
    "                  'soil_geo': soil_and_geo_variables,\n",
    "                  'urban': urban_variables}\n",
    "\n",
    "    for key, values in save_names.items():\n",
    "        df_to_disk[values].to_csv('{path}/{fn}.csv'.format(path=path_to_save,\n",
    "                                                           fn=key), sep=';')\n",
    "\n",
    "    return\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "save_results(eXtracted_data=output,\n",
    "             gauge_ids=big_shape['gauge_id'],\n",
    "             path_to_save=path_to_save)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('geo': conda)"
  },
  "interpreter": {
   "hash": "6c1631cd9b0ec259f0745443ebae1d32df81b6c8f8981e3ac9a0ef143415de84"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}