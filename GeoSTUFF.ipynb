{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e556c8de-958f-4d22-98c2-a733cb58c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72babd7-e566-410b-b813-03de74a42c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_file_path = '/mnt/d/education/HSI/aspirantura/CAMELS_ru/files/openf_gauges_watersheds/watersheds_openf.shp'\n",
    "gdb_file_path = '/mnt/d/education/HSI/aspirantura/CAMELS_ru/files/HydroAtlas/Data/BasinATLAS_v10.gdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f9cbaca-b56d-462d-9176-86cfeaea7c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name_en</th>\n",
       "      <th>geometry</th>\n",
       "      <th>area_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48075.0</td>\n",
       "      <td>R.VODLA - D.VODLA</td>\n",
       "      <td>MULTIPOLYGON (((36.51542 62.36375, 36.51542 62...</td>\n",
       "      <td>8010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48079.0</td>\n",
       "      <td>R.VODLA - G.PUDOJ</td>\n",
       "      <td>MULTIPOLYGON (((37.72375 61.47875, 37.72375 61...</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72043.0</td>\n",
       "      <td>R.TOSNA - ST.TOSNO</td>\n",
       "      <td>MULTIPOLYGON (((30.59375 59.08708, 30.59375 59...</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72155.0</td>\n",
       "      <td>R.PASA - NIJE D.DUBROVO</td>\n",
       "      <td>MULTIPOLYGON (((33.62375 59.87458, 33.62375 59...</td>\n",
       "      <td>3910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72156.0</td>\n",
       "      <td>R.PASA - S.CASOVENSKOE</td>\n",
       "      <td>MULTIPOLYGON (((33.05458 59.96792, 33.05375 59...</td>\n",
       "      <td>5710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>6329.0</td>\n",
       "      <td>r.Unaha - s.Unaha</td>\n",
       "      <td>POLYGON ((126.68567 55.67045, 126.68567 55.669...</td>\n",
       "      <td>1950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>4005.0</td>\n",
       "      <td>r. Ohinka - g. Oha</td>\n",
       "      <td>POLYGON ((142.93984 53.55962, 142.93650 53.559...</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>5674.0</td>\n",
       "      <td>r.Sukinka 1-a - 13 km Birsosse</td>\n",
       "      <td>POLYGON ((132.77067 48.75212, 132.77067 48.751...</td>\n",
       "      <td>59.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>6564.0</td>\n",
       "      <td>No name</td>\n",
       "      <td>POLYGON ((131.57067 51.38879, 131.57150 51.388...</td>\n",
       "      <td>2990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>3431.0</td>\n",
       "      <td>r.Tinege - GMS Ekuccu</td>\n",
       "      <td>POLYGON ((131.32131 66.70605, 131.31465 66.706...</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                         name_en  \\\n",
       "0     48075.0               R.VODLA - D.VODLA   \n",
       "1     48079.0               R.VODLA - G.PUDOJ   \n",
       "2     72043.0              R.TOSNA - ST.TOSNO   \n",
       "3     72155.0         R.PASA - NIJE D.DUBROVO   \n",
       "4     72156.0          R.PASA - S.CASOVENSKOE   \n",
       "...       ...                             ...   \n",
       "1025   6329.0               r.Unaha - s.Unaha   \n",
       "1026   4005.0              r. Ohinka - g. Oha   \n",
       "1027   5674.0  r.Sukinka 1-a - 13 km Birsosse   \n",
       "1028   6564.0                         No name   \n",
       "1029   3431.0           r.Tinege - GMS Ekuccu   \n",
       "\n",
       "                                               geometry   area_1  \n",
       "0     MULTIPOLYGON (((36.51542 62.36375, 36.51542 62...   8010.0  \n",
       "1     MULTIPOLYGON (((37.72375 61.47875, 37.72375 61...  12000.0  \n",
       "2     MULTIPOLYGON (((30.59375 59.08708, 30.59375 59...   1300.0  \n",
       "3     MULTIPOLYGON (((33.62375 59.87458, 33.62375 59...   3910.0  \n",
       "4     MULTIPOLYGON (((33.05458 59.96792, 33.05375 59...   5710.0  \n",
       "...                                                 ...      ...  \n",
       "1025  POLYGON ((126.68567 55.67045, 126.68567 55.669...   1950.0  \n",
       "1026  POLYGON ((142.93984 53.55962, 142.93650 53.559...     13.2  \n",
       "1027  POLYGON ((132.77067 48.75212, 132.77067 48.751...     59.1  \n",
       "1028  POLYGON ((131.57067 51.38879, 131.57150 51.388...   2990.0  \n",
       "1029  POLYGON ((131.32131 66.70605, 131.31465 66.706...    115.0  \n",
       "\n",
       "[1030 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_shape = gpd.read_file(shape_file_path)\n",
    "big_shape = big_shape[['code', 'name_en', 'geometry', 'area_1']]\n",
    "big_shape = big_shape.rename(columns={\"code\": \"ID\"})\n",
    "big_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6bfef-9fb0-4ac0-8339-d6e486901883",
   "metadata": {},
   "source": [
    "#### Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4853e1d-8d7a-4d96-bca8-4a36c97e939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_area(lats, lons, radius = 6378137):\n",
    "    \"\"\"\n",
    "    Computes area of spherical polygon, assuming spherical Earth. \n",
    "    Returns result in ratio of the sphere's area if the radius is specified.\n",
    "    Otherwise, in the units of provided radius.\n",
    "    lats and lons are in degrees.\n",
    "    \"\"\"\n",
    "    from numpy import arctan2, cos, sin, sqrt, pi, power, append, diff, deg2rad\n",
    "    lats, lons = np.deg2rad(lats), np.deg2rad(lons)\n",
    "\n",
    "    # Line integral based on Green's Theorem, assumes spherical Earth\n",
    "\n",
    "    #close polygon\n",
    "    if lats[0]!=lats[-1]:\n",
    "        lats=append(lats, lats[0])\n",
    "        lons=append(lons, lons[0])\n",
    "\n",
    "    #colatitudes relative to (0,0)\n",
    "    a = sin(lats/2)**2 + cos(lats)* sin(lons/2)**2\n",
    "    colat = 2*arctan2( sqrt(a), sqrt(1-a) )\n",
    "\n",
    "    #azimuths relative to (0,0)\n",
    "    az = arctan2(cos(lats) * sin(lons), sin(lats)) % (2*pi)\n",
    "\n",
    "    # Calculate diffs\n",
    "    # daz = diff(az) % (2*pi)\n",
    "    daz = diff(az)\n",
    "    daz = (daz + pi) % (2 * pi) - pi\n",
    "\n",
    "    deltas=diff(colat)/2\n",
    "    colat=colat[0:-1]+deltas\n",
    "\n",
    "    # Perform integral\n",
    "    integrands = (1-cos(colat)) * daz\n",
    "\n",
    "    # Integrate \n",
    "    area = abs(sum(integrands))/(4*pi)\n",
    "\n",
    "    area = min(area,1-area)\n",
    "    if radius is not None: #return in units of radius\n",
    "        return area * 4 * pi* radius**2 / 10**6\n",
    "    else: #return in ratio of sphere total area\n",
    "        return area / 10**6\n",
    "    \n",
    "def split_by_categories(df_ecm, df_me, df_mo):\n",
    "    \n",
    "    # basic numbers for different variables\n",
    "    monthes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    land_cover_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']\n",
    "    natural_vegetation = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "    wetland_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
    "    \n",
    "    hydrology_variables = [item for sublist in [['inu_pc_ult'], ['lka_pc_use'], ['lkv_mc_usu'],\n",
    "                                            ['rev_mc_usu'], ['dor_pc_pva'], ['gwt_cm_sav']]\n",
    "                    for item in sublist]\n",
    "\n",
    "    physiography_variables = [item for sublist in [['ele_mt_sav'], ['slp_dg_sav'], ['sgr_dk_sav']] \n",
    "                            for item in sublist]\n",
    "\n",
    "    climate_variables = [item for sublist in [['clz_cl_smj'], ['cls_cl_smj'], ['tmp_dc_s{}'.format(i) for i in monthes], \n",
    "                                            ['pre_mm_s{}'.format(i) for i in monthes], ['pet_mm_s{}'.format(i) for i in monthes],\n",
    "                                            ['aet_mm_s{}'.format(i) for i in monthes], ['ari_ix_sav'],\n",
    "                                            ['cmi_ix_s{}'.format(i) for i in monthes], ['snw_pc_s{}'.format(i) for i in monthes]] \n",
    "                        for item in sublist]\n",
    "\n",
    "    landcover_variables = [item for sublist in [['glc_cl_smj'], ['glc_pc_s{}'.format(i) for i in land_cover_classes], \n",
    "                                                ['pnv_cl_smj'], ['wet_cl_smj'], ['wet_pc_s{}'.format(i) for i in wetland_classes],\n",
    "                                                ['for_pc_sse'], ['crp_pc_sse'], ['pst_pc_sse'], \n",
    "                                                ['ire_pc_sse'], ['gla_pc_sse'], ['prm_pc_sse'], \n",
    "                                                ['tbi_cl_smj'], ['tec_cl_smj']]\n",
    "                        for item in sublist]\n",
    "\n",
    "    soil_and_geo_variables = [item for sublist in [['cly_pc_sav'], ['slt_pc_sav'], ['snd_pc_sav'], \n",
    "                                                ['soc_th_sav'], ['swc_pc_syr'], ['swc_pc_s{}'.format(i) for i in monthes],\n",
    "                                                ['lit_cl_smj'], ['kar_pc_sse'], ['ero_kh_sav']]\n",
    "                            for item in sublist]\n",
    "\n",
    "    urban_variables = [item for sublist in [['urb_pc_sse'], ['hft_ix_s93'], ['hft_ix_s09']] for item in sublist]\n",
    "\n",
    "    # dataframe of hydrology variables\n",
    "    df_HYDRO = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in hydrology_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in hydrology_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in hydrology_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "    # dataframe of physiography variables\n",
    "    df_PHYSIO = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in physiography_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in physiography_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in physiography_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "\n",
    "    # dataframe of climate variables\n",
    "    df_CLIMATE = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in climate_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in climate_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in climate_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "    # dataframe of physiography variables                       \n",
    "    df_LANDCOVER = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in landcover_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in landcover_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in landcover_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "    # dataframe of soil and geology variables\n",
    "    df_SOIL_GEO = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in soil_and_geo_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in soil_and_geo_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in soil_and_geo_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "    # dataframe of urban variables\n",
    "    df_URBAN = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in urban_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in urban_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in urban_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "    return [df_HYDRO, df_PHYSIO, df_CLIMATE, df_LANDCOVER, df_SOIL_GEO, df_URBAN]\n",
    "    \n",
    "\n",
    "def filter_HydroATLAS_sub_basins(WS_own, HydroATLAS_data):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    WS_own - Watershed from your GDF of watersheds\n",
    "    HydroATLAS_data - gdf file from layers of geodatabase\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    intersected_sub_basins = list()\n",
    "\n",
    "    def select_big_from_MP(WS_geometry):\n",
    "        if type(WS_geometry) == MultiPolygon:\n",
    "            big_area = [polygon_area(lats = polygon.exterior.coords.xy[1], \n",
    "                                    lons = polygon.exterior.coords.xy[0]) \n",
    "                        for polygon in WS_geometry]\n",
    "            import numpy as np\n",
    "            WS_geometry = WS_geometry[np.argmax(big_area)]\n",
    "        else:\n",
    "            WS_geometry = WS_geometry\n",
    "        return WS_geometry\n",
    "\n",
    "    gdf_your_WS = select_big_from_MP(WS_own)\n",
    "    ### WS from your data\n",
    "    gdf_your_WS = gpd.GeoSeries([gdf_your_WS])\n",
    "\n",
    "    ### Create extra gdf to use geopandas functions\n",
    "    gdf_your_WS = gpd.GeoDataFrame({'geometry': gdf_your_WS})\n",
    "    gdf_your_WS = gdf_your_WS.set_crs('EPSG:4326')\n",
    "\n",
    "    for HydroATLAS_row in range(len(HydroATLAS_data)):\n",
    "\n",
    "        # selection from sub-basins of GeoDataBase\n",
    "        HydroATLAS_WS = gpd.GeoSeries(select_big_from_MP(HydroATLAS_data.geometry[HydroATLAS_row]))        \n",
    "\n",
    "        gdf_HydroATLAS_WS = gpd.GeoDataFrame({'geometry': HydroATLAS_WS}).set_crs('EPSG:4326')\n",
    "\n",
    "        #intersect basins\n",
    "        res_intersection = gpd.overlay(gdf_your_WS, gdf_HydroATLAS_WS, how='intersection')\n",
    "\n",
    "        \"\"\"\n",
    "        Check if our intersection between sub-basin form HydroAtlas and our watershed is more than 0.6 of \n",
    "        sub-basin itself\n",
    "        If not - than pass        \n",
    "        \"\"\"\n",
    "        if len(res_intersection) != 0:\n",
    "            res_intersection = select_big_from_MP(res_intersection.geometry[0])\n",
    "\n",
    "            if polygon_area(lats = res_intersection.exterior.coords.xy[1], \n",
    "                            lons = res_intersection.exterior.coords.xy[0])/polygon_area(lats = gdf_HydroATLAS_WS.geometry[0].exterior.coords.xy[1], \n",
    "                                                                                                lons = gdf_HydroATLAS_WS.geometry[0].exterior.coords.xy[0]) > 0.2:\n",
    "                    \n",
    "\n",
    "                    intersected_sub_basins.append(HydroATLAS_data.loc[HydroATLAS_row])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return intersected_sub_basins\n",
    "\n",
    "\n",
    "def get_HydroATLAS_for_WS(WS, WS_index, path_to_HydroATLAS, layers_from_HydroATLAS):\n",
    "\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "    def select_big_from_MP(WS_geometry):\n",
    "        if type(WS_geometry) == MultiPolygon:\n",
    "            big_area = [polygon_area(lats = polygon.exterior.coords.xy[1], \n",
    "                                    lons = polygon.exterior.coords.xy[0]) \n",
    "                        for polygon in WS_geometry]\n",
    "            WS_geometry = WS_geometry[np.argmax(big_area)]\n",
    "        else:\n",
    "            WS_geometry = WS_geometry\n",
    "        return WS_geometry\n",
    "    \n",
    "    \"\"\"                             \n",
    "    WS - Your data with watershed GDF.geometry[:]\n",
    "    WS_index - Number of the index of WS from GeoDataFrame geometry field\n",
    "    path_to_HydroATLAS - Path to BasinATLAS_v10.gdb file\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # basic numbers for different variables\n",
    "    monthes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    land_cover_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']\n",
    "    natural_vegetation = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "    wetland_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
    "    \n",
    "    # Get all the layers from the .gdb file \n",
    "    layers = layers_from_HydroATLAS\n",
    "    # -1 layer - high density sub-basins (lowest area)\n",
    "    \n",
    "    # Read choosen geodatabase layer with geopandas\n",
    "    gdf = gpd.read_file(path_to_HydroATLAS, \n",
    "                        mask = WS.geometry[WS_index], layer=layers,  ignore_geometry=False)\n",
    "    \n",
    "    \n",
    "    list_of_goodies = filter_HydroATLAS_sub_basins(WS.geometry[WS_index], gdf)\n",
    "  \n",
    "    if len(list_of_goodies) != 0:\n",
    "        list_of_goodies = gpd.GeoDataFrame(pd.DataFrame(list_of_goodies)).set_crs('EPSG:4326').reset_index(drop = True)\n",
    "        from shapely.ops import unary_union\n",
    "        union_geometry = gpd.GeoSeries(unary_union([i for i in list_of_goodies.geometry])).set_crs('epsg:4326')\n",
    "        union_geometry = select_big_from_MP(union_geometry.geometry[0])\n",
    "\n",
    "        \"\"\"\n",
    "        group columns by category (difference is the way of mathematical aggregation)\n",
    "\n",
    "        e.g. classes will be aggrgated by mode value in sub-basins of the watershed\n",
    "\n",
    "        other values will be calculated as a mean for selected watershed\n",
    "\n",
    "        \"\"\"\n",
    "        # values which will be aggregated by mean\n",
    "        columns_MEAN = [['inu_pc_ult'], ['lka_pc_use'], ['lkv_mc_usu'], ['rev_mc_usu'], ['dor_pc_pva'], ['gwt_cm_sav'], ['ele_mt_sav'], ['slp_dg_sav'],\n",
    "                ['sgr_dk_sav'], ['tmp_dc_s{}'.format(i) for i in monthes], ['pre_mm_s{}'.format(i) for i in monthes], \n",
    "                ['pet_mm_s{}'.format(i) for i in monthes], ['aet_mm_s{}'.format(i) for i in monthes], ['snw_pc_s{}'.format(i) for i in monthes], \n",
    "                ['glc_pc_s{}'.format(i) for i in land_cover_classes], ['pnv_pc_s{}'.format(i) for i in natural_vegetation], ['wet_pc_s{}'.format(i) for i in wetland_classes], \n",
    "                ['for_pc_sse'], ['crp_pc_sse'], ['pst_pc_sse'], ['ire_pc_sse'], ['gla_pc_sse'], ['prm_pc_sse'], ['cly_pc_sav'], ['slt_pc_sav'], \n",
    "                ['snd_pc_sav'], ['soc_th_sav'], ['swc_pc_syr'], ['swc_pc_s{}'.format(i) for i in monthes], ['kar_pc_sse'], ['ero_kh_sav'], ['urb_pc_sse']]\n",
    "\n",
    "        # values which will be aggregated by mode\n",
    "        columns_MODE = [['clz_cl_smj'], ['cls_cl_smj'], ['glc_cl_smj'], ['pnv_cl_smj'],\n",
    "                        ['wet_cl_smj'], ['tbi_cl_smj'], ['tec_cl_smj'], ['lit_cl_smj']]\n",
    "\n",
    "        # values which will be aggregated by mean but need extra calculations\n",
    "        # e.g. ari_ix need to be divided by 10, cmi by 100 etc.\n",
    "        # for some reason values exceed treshold of the range\n",
    "        columns_EXTRA_CALC_MEAN = [['ari_ix_sav'], ['cmi_ix_s{}'.format(i) for i in monthes], ['hft_ix_s93'], ['hft_ix_s09']]\n",
    "\n",
    "        # split list of lists to needed columns\n",
    "        columns_EXTRA_CALC_MEAN = [item for sublist in columns_EXTRA_CALC_MEAN for item in sublist]\n",
    "        columns_MEAN = [item for sublist in columns_MEAN for item in sublist]\n",
    "        columns_MODE = [item for sublist in columns_MODE for item in sublist]\n",
    "        \n",
    "        # dataframe for indexes\n",
    "\n",
    "        df_EXTRA_CALC_MEAN = list_of_goodies[columns_EXTRA_CALC_MEAN]\n",
    "        df_EXTRA_CALC_MEAN.loc[:, ['ari_ix_sav']] /= 10 # aridity index is the value between 0 and 100. In current version of HydroATLAS (v 1.0) it's vary between 0 and 1000\n",
    "        \n",
    "        df_EXTRA_CALC_MEAN.loc[:, ['cmi_ix_s{}'.format(i) for i in monthes]] /= 100 # aridity index is the value between -1 and 1. In current version of HydroATLAS (v 1.0) it's vary between -100 and 100\n",
    "\n",
    "        df_EXTRA_CALC_MEAN = df_EXTRA_CALC_MEAN.mean()\n",
    "        \n",
    "        # dataframe for area values\n",
    "\n",
    "        df_MEAN = list_of_goodies[columns_MEAN]\n",
    "        df_MEAN.loc[:, ['tmp_dc_s{}'.format(i) for i in monthes]] /= 10 # in some regions on North-West Russia average value for Jan -83. I assume it's need to be divide by 10\n",
    "        df_MEAN = df_MEAN.mean()\n",
    "        \n",
    "\n",
    "        #dataframe for classes\n",
    "\n",
    "        df_MODE = list_of_goodies[columns_MODE]\n",
    "        df_MODE = df_MODE.replace(-9999, np.NaN) # Это вопрос: может стоит оставить \"отсутствующий класс\" \"мокрых земель\"\n",
    "        df_MODE = df_MODE.mode()\n",
    "        \n",
    "        \n",
    "        list_of_frames = [df_EXTRA_CALC_MEAN, df_MEAN, df_MODE]\n",
    "        \n",
    "        for i in range(len(list_of_frames)):\n",
    "            if type(list_of_frames[i]) == pd.Series:\n",
    "                list_of_frames[i] = list_of_frames[i].to_frame().T\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        fin = split_by_categories(df_ecm = list_of_frames[0], df_me = list_of_frames[1], df_mo = list_of_frames[2])\n",
    "    \n",
    "    else:\n",
    "        list_of_goodies = np.NaN\n",
    "        union_geometry = np.NaN\n",
    "        fin = np.NaN \n",
    "    \n",
    "    \n",
    "    return fin, union_geometry, list_of_goodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c600923-bc22-48a7-99c1-6d7f45f3e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_big_shape(shape):\n",
    "\n",
    "    list_of_shapes = [shape[i*500:(i+1)*500].reset_index(drop=True)\n",
    "                      for i in range(len(shape)//500 + 1)]\n",
    "\n",
    "    return list_of_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a65fa6f-3049-4f73-8b08-a349ca51ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_function(WS, path_to_HydroATLAS, layer_small):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function generate list of tuples\n",
    "    where each tuple stands for row in DF\n",
    "    of watersheds\n",
    "    WS - GeoDataFrame of WS\n",
    "    path_to_HydroATLAS - path to BasinATLAS gdb\n",
    "    layer_small - fiona layer of smallest grid of WS    \n",
    "    \"\"\"\n",
    "    mp_tuples = list()\n",
    "    path_to_HydroATLAS = path_to_HydroATLAS\n",
    "    \n",
    "    for row in range(len(WS)):\n",
    "        mp_tuples.append((WS,\n",
    "                          row,\n",
    "                          path_to_HydroATLAS,\n",
    "                          layer_small))\n",
    "    \n",
    "    return mp_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdfb5524-796b-4e8a-83df-03960567b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_small = fiona.listlayers(gdb_file_path)[-1]\n",
    "# WS, WS_index, path_to_HydroATLAS, layers_from_HydroATLAS\n",
    "data = parallelize_function(WS = big_shape,\n",
    "                            path_to_HydroATLAS = gdb_file_path,\n",
    "                            layer_small=layer_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f2e8832-8413-4c3f-b027-cff2563e792c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff468ec9b6f14a5fa9ebca4c7aded738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data and iterations on test example\n",
    "\n",
    "# iters = len(data)\n",
    "# _MAX_ITERATIONS = iters\n",
    "# data = data[:iters]\n",
    "\n",
    "# run test prallel example\n",
    "\n",
    "function_processors = mp.cpu_count()//2\n",
    "process_pool = mp.Pool(function_processors)\n",
    "# WS, WS_index, path_to_HydroATLAS, layers_from_HydroATLAS\n",
    "output = process_pool.starmap(get_HydroATLAS_for_WS, tq.tqdm(data))\n",
    "process_pool.close()\n",
    "process_pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c48e370-fba5-4db9-bb5f-ae16bcda5d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_geo_files_to_disk(shape_with_data, list_of_values, path_to_save):\n",
    "    \n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.makedirs(path_to_save)\n",
    "    \n",
    "    bool_array = list()\n",
    "    \n",
    "    for i in range(len(list_of_values)):\n",
    "        if type(list_of_values[i][0]) == float:\n",
    "            bool_array.append(False)\n",
    "        else:\n",
    "            bool_array.append(True)\n",
    "            \n",
    "    VALID_ID = [ID for i, ID in enumerate(shape_with_data.ID) if bool_array[i]]\n",
    "\n",
    "    VALID_HYDRO = [hydro[0][0] for i, hydro in enumerate(list_of_values) if bool_array[i]]\n",
    "    hydro_df = pd.concat(VALID_HYDRO).dropna().reset_index(drop = True)\n",
    "    hydro_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    hydro_df.to_csv('{}/hydro.csv'.format(path_to_save), index = False)\n",
    "\n",
    "    VALID_PHYSIO = [physio[0][1] for i, physio in enumerate(list_of_values) if bool_array[i]]\n",
    "    physio_df = pd.concat(VALID_PHYSIO).dropna().reset_index(drop = True)\n",
    "    physio_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    physio_df.to_csv('{}/physio.csv'.format(path_to_save), index = False)\n",
    "\n",
    "    VALID_CLIMATE = [climate[0][2] for i, climate in enumerate(list_of_values) if bool_array[i]]\n",
    "    climate_df = pd.concat(VALID_CLIMATE).dropna().reset_index(drop = True)\n",
    "    climate_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    climate_df.to_csv('{}/climate.csv'.format(path_to_save), index = False)\n",
    "\n",
    "    VALID_URBAN = [urban[0][5] for i, urban in enumerate(list_of_values) if bool_array[i]]\n",
    "    urban_df = pd.concat(VALID_URBAN).dropna().reset_index(drop = True)\n",
    "    urban_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    urban_df.to_csv('{}/urban.csv'.format(path_to_save), index = False)\n",
    "\n",
    "    VALID_LANDCOVER = [landcover[0][3] for i, landcover in enumerate(list_of_values) if bool_array[i]] #because of big empty sets of WS Landcover is seleceting by indexes of urban values which are verified\n",
    "    landcover_df = pd.concat(VALID_LANDCOVER).dropna(thresh = 5).reset_index(drop = True)\n",
    "    landcover_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    landcover_df.to_csv('{}/landcover.csv'.format(path_to_save), index = False)\n",
    "\n",
    "    VALID_SOIL_GEO = [soil_geo[0][4] for i, soil_geo in enumerate(list_of_values) if bool_array[i]]\n",
    "    soil_geo_df = pd.concat(VALID_SOIL_GEO).dropna().reset_index(drop = True)\n",
    "    soil_geo_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    soil_geo_df.to_csv('{}/soil_geo.csv'.format(path_to_save), index = False)\n",
    "\n",
    "    VALID_GEOM_HydroATLAS = [geometry[1] for i, geometry in enumerate(list_of_values) if bool_array[i]]\n",
    "    geometry_df = gpd.GeoDataFrame(VALID_GEOM_HydroATLAS,\n",
    "                                   columns={'geometry'}).reset_index(drop = True)\n",
    "    geometry_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "\n",
    "    geometry_df.to_csv('{}/geometry_HydroATLAS_subB.csv'.format(path_to_save), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c1f2ae-a413-4380-9de6-7cddce95b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_geo_files_to_disk(shape_with_data=big_shape, \n",
    "                       list_of_values=output,\n",
    "                       path_to_save='/mnt/d/education/HSI/aspirantura/CAMELS_ru/test/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
